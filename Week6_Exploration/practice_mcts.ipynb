{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "name": "practice_mcts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pakshingho/Practical-Reinforcement-Learning-HSE/blob/main/Week6_Exploration/practice_mcts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqS7qCKRcWvp"
      },
      "source": [
        "## Seminar: Monte-carlo tree search (5 pts)\n",
        "\n",
        "Monte Carlo tree search (MCTS) is a heuristic search algorithm, which shows cool results in challenging domains such as Go and chess. The algorithm builds a search tree, iteratively traverses it, and evaluates its nodes using a Monte-Carlo simulation.\n",
        "\n",
        "In this seminar, we'll implement a MCTS([[1]](#1), [[2]](#2)) planning and use it to solve some Gym envs.\n",
        "\n",
        "![image.png](https://i.postimg.cc/6QmwnjPS/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgG9iLUecWvs"
      },
      "source": [
        "__How it works?__\n",
        "We just start with an empty tree and expand it. There are several common procedures.\n",
        "\n",
        "__1) Selection__\n",
        "Starting from the root, recursively select the node that corresponds to the tree policy.  \n",
        "\n",
        "There are several options for tree policies, which we saw earlier as exploration strategies: epsilon-greedy, Thomson sampling, UCB-1. It was shown that in MCTS, UCB-1 achieves a good result. Further, we will consider the one, but you can try to use others.\n",
        "\n",
        "Following the UCB-1 tree policy, we will choose an action that, on one hand, we expect to have the highest return, and on the other hand, we haven't explored much.\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dot{a} = \\argmax_{a} \\dot{Q}(s, a)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dot{Q}(s, a) = Q(s, a) + C_p \\sqrt{\\frac{2 \\log {N}}{n_a}}\n",
        "$$\n",
        "\n",
        "where: \n",
        "- $N$ - number of times we have visited state $s$,\n",
        "- $n_a$ - number of times we have taken action $a$,\n",
        "- $C_p$ - exploration balance parameter, which is performed between exploration and exploitation. \n",
        "\n",
        "Using Hoeffding inequality for rewards $R \\in [0,1]$ it can be shown [[3]](#3) that optimal $C_p = 1/\\sqrt{2}$. For rewards outside this range, the parameter should be tuned. We'll be using 10, but you can experiment with other values.\n",
        "\n",
        "__2) Expansion__\n",
        "After the selection procedure, we can achieve a leaf node or node in which we don't complete actions. In this case, we expand the tree by feasible actions and get new state nodes. \n",
        "\n",
        "__3) Simulation__\n",
        "How we can estimate node Q-values? The idea is to estimate action values for a given _rollout policy_ by averaging the return of many simulated trajectories from the current node. Simply, we can play with random or some special policy or use some model that can estimate it.\n",
        "\n",
        "__4) Backpropagation__\n",
        "The reward of the last simulation is backed up through the traversed nodes and propagates Q-value estimations, upwards to the root.\n",
        "\n",
        "$$\n",
        "Q({\\text{parent}}, a) = r + \\gamma \\cdot Q({\\text{child}}, a)\n",
        "$$\n",
        "\n",
        "There are a lot modifications of MCTS, more details about it you can find in this paper [[4]](#4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN62RSCfcWvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e66832e-7f6f-4d4c-e7a8-ad22cf58022b"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5pCtUo-cWvu"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4g7Pv6cWvv"
      },
      "source": [
        "---\n",
        "\n",
        "But before we do that, we first need to make a wrapper for Gym environments to allow saving and loading game states to facilitate backtracking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3MJQlQScWvw"
      },
      "source": [
        "import gym\n",
        "from gym.core import Wrapper\n",
        "from pickle import dumps, loads\n",
        "from collections import namedtuple\n",
        "\n",
        "# a container for get_result function below. Works just like tuple, but prettier\n",
        "ActionResult = namedtuple(\n",
        "    \"action_result\", (\"snapshot\", \"observation\", \"reward\", \"is_done\", \"info\"))\n",
        "\n",
        "\n",
        "class WithSnapshots(Wrapper):\n",
        "    \"\"\"\n",
        "    Creates a wrapper that supports saving and loading environemnt states.\n",
        "    Required for planning algorithms.\n",
        "\n",
        "    This class will have access to the core environment as self.env, e.g.:\n",
        "    - self.env.reset()           #reset original env\n",
        "    - self.env.ale.cloneState()  #make snapshot for atari. load with .restoreState()\n",
        "    - ...\n",
        "\n",
        "    You can also use reset() and step() directly for convenience.\n",
        "    - s = self.reset()                   # same as self.env.reset()\n",
        "    - s, r, done, _ = self.step(action)  # same as self.env.step(action)\n",
        "    \n",
        "    Note that while you may use self.render(), it will spawn a window that cannot be pickled.\n",
        "    Thus, you will need to call self.close() before pickling will work again.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_snapshot(self, render=False):\n",
        "        \"\"\"\n",
        "        :returns: environment state that can be loaded with load_snapshot \n",
        "        Snapshots guarantee same env behaviour each time they are loaded.\n",
        "\n",
        "        Warning! Snapshots can be arbitrary things (strings, integers, json, tuples)\n",
        "        Don't count on them being pickle strings when implementing MCTS.\n",
        "\n",
        "        Developer Note: Make sure the object you return will not be affected by \n",
        "        anything that happens to the environment after it's saved.\n",
        "        You shouldn't, for example, return self.env. \n",
        "        In case of doubt, use pickle.dumps or deepcopy.\n",
        "\n",
        "        \"\"\"\n",
        "        if render:\n",
        "            self.render()  # close popup windows since we can't pickle them\n",
        "            self.close()\n",
        "            \n",
        "        if self.unwrapped.viewer is not None:\n",
        "            self.unwrapped.viewer.close()\n",
        "            self.unwrapped.viewer = None\n",
        "        return dumps(self.env)\n",
        "\n",
        "    def load_snapshot(self, snapshot, render=False):\n",
        "        \"\"\"\n",
        "        Loads snapshot as current env state.\n",
        "        Should not change snapshot inplace (in case of doubt, deepcopy).\n",
        "        \"\"\"\n",
        "\n",
        "        assert not hasattr(self, \"_monitor\") or hasattr(\n",
        "            self.env, \"_monitor\"), \"can't backtrack while recording\"\n",
        "\n",
        "        if render:\n",
        "            self.render()  # close popup windows since we can't load into them\n",
        "            self.close()\n",
        "        self.env = loads(snapshot)\n",
        "\n",
        "    def get_result(self, snapshot, action):\n",
        "        \"\"\"\n",
        "        A convenience function that \n",
        "        - loads snapshot, \n",
        "        - commits action via self.step,\n",
        "        - and takes snapshot again :)\n",
        "\n",
        "        :returns: next snapshot, next_observation, reward, is_done, info\n",
        "\n",
        "        Basically it returns next snapshot and everything that env.step would have returned.\n",
        "        \"\"\"\n",
        "\n",
        "        # <YOUR CODE: load, commit, take snapshot>\n",
        "        self.load_snapshot(snapshot)\n",
        "        s, r, done, info = self.step(action)\n",
        "        next_snapshot = self.get_snapshot()\n",
        "\n",
        "        return ActionResult(\n",
        "            next_snapshot,\n",
        "            s,\n",
        "            r,\n",
        "            done,\n",
        "            info\n",
        "        )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV7I1VtccWvy"
      },
      "source": [
        "### Try out snapshots:\n",
        "Let`s check our wrapper. At first, reset environment and save it, further randomly play some actions and restore our environment from the snapshot. It should be the same as our previous initial state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tas89DErcWvy"
      },
      "source": [
        "# make env\n",
        "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYfWjztScWvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "e4a6b02e-9388-4276-8836-f7adb20ab24a"
      },
      "source": [
        "print(\"initial_state:\")\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "env.close()\n",
        "\n",
        "# create first snapshot\n",
        "snap0 = env.get_snapshot()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_state:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcZVGiJcWvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "440835c2-7b9e-4323-a747-30c58669d44c"
      },
      "source": [
        "# play without making snapshots (faster)\n",
        "while True:\n",
        "    is_done = env.step(env.action_space.sample())[2]\n",
        "    if is_done:\n",
        "        print(\"Whoops! We died!\")\n",
        "        break\n",
        "\n",
        "print(\"final state:\")\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "env.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Whoops! We died!\n",
            "final state:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCviCg2KcWv0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "2a405f79-60b4-4957-a8f5-47ce4a32b355"
      },
      "source": [
        "# reload initial state\n",
        "env.load_snapshot(snap0)\n",
        "\n",
        "print(\"\\n\\nAfter loading snapshot\")\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "env.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "After loading snapshot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8NzS5rNcWv0"
      },
      "source": [
        "# get outcome (snapshot, observation, reward, is_done, info)\n",
        "res = env.get_result(snap0, env.action_space.sample())\n",
        "\n",
        "snap1, observation, reward = res[:3]\n",
        "\n",
        "# second step\n",
        "res2 = env.get_result(snap1, env.action_space.sample())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSW36ikFcWv1"
      },
      "source": [
        "# MCTS: Monte-Carlo tree search\n",
        "\n",
        "\n",
        "We will start by implementing the `Node` class - a simple class that acts like MCTS node and supports some of the MCTS algorithm steps.\n",
        "\n",
        "This MCTS implementation makes some assumptions about the environment, you can find those _in the notes section at the end of the notebook_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT3IT_H3cWv1"
      },
      "source": [
        "assert isinstance(env, WithSnapshots)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxiRz-cKcWv2"
      },
      "source": [
        "class Node:\n",
        "    \"\"\"A tree node for MCTS.\n",
        "    \n",
        "    Each Node corresponds to the result of performing a particular action (self.action)\n",
        "    in a particular state (self.parent), and is essentially one arm in the multi-armed bandit that\n",
        "    we model in that state.\"\"\"\n",
        "\n",
        "    # metadata:\n",
        "    parent = None  # parent Node\n",
        "    qvalue_sum = 0.  # sum of Q-values from all visits (numerator)\n",
        "    times_visited = 0  # counter of visits (denominator)\n",
        "\n",
        "    def __init__(self, parent, action):\n",
        "        \"\"\"\n",
        "        Creates and empty node with no children.\n",
        "        Does so by commiting an action and recording outcome.\n",
        "\n",
        "        :param parent: parent Node\n",
        "        :param action: action to commit from parent Node\n",
        "        \"\"\"\n",
        "\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = set()  # set of child nodes\n",
        "\n",
        "        # get action outcome and save it\n",
        "        res = env.get_result(parent.snapshot, action)\n",
        "        self.snapshot, self.observation, self.immediate_reward, self.is_done, _ = res\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def is_root(self):\n",
        "        return self.parent is None\n",
        "\n",
        "    def get_qvalue_estimate(self):\n",
        "        return self.qvalue_sum / self.times_visited if self.times_visited != 0 else 0\n",
        "\n",
        "    def ucb_score(self, scale=10, max_value=1e100):\n",
        "        \"\"\"\n",
        "        Computes ucb1 upper bound using current value and visit counts for node and it's parent.\n",
        "\n",
        "        :param scale: Multiplies upper bound by that. From Hoeffding inequality,\n",
        "                      assumes reward range to be [0, scale].\n",
        "        :param max_value: a value that represents infinity (for unvisited nodes).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if self.times_visited == 0:\n",
        "            return max_value\n",
        "\n",
        "        # compute ucb-1 additive component (to be added to mean value)\n",
        "        # hint: you can use self.parent.times_visited for N times node was considered,\n",
        "        # and self.times_visited for n times it was visited\n",
        "\n",
        "        # U = <YOUR CODE>\n",
        "        U = np.sqrt(2*np.log(self.parent.times_visited)/self.times_visited)\n",
        "\n",
        "        return self.get_qvalue_estimate() + scale * U\n",
        "\n",
        "    # MCTS steps\n",
        "\n",
        "    def select_best_leaf(self):\n",
        "        \"\"\"\n",
        "        Picks the leaf with the highest priority to expand.\n",
        "        Does so by recursively picking nodes with the best UCB-1 score until it reaches a leaf.\n",
        "        \"\"\"\n",
        "        if self.is_leaf():\n",
        "            return self\n",
        "\n",
        "        children = list(self.children)\n",
        "\n",
        "        # Select the child node with the highest UCB score. You might want to implement some heuristics\n",
        "        # to break ties in a smart way, although CartPole should work just fine without them.\n",
        "        # best_child = <YOUR CODE>\n",
        "        best_child = children[np.argmax([child.ucb_score() for child in children])]\n",
        "\n",
        "\n",
        "        return best_child.select_best_leaf()\n",
        "\n",
        "    def expand(self):\n",
        "        \"\"\"\n",
        "        Expands the current node by creating all possible child nodes.\n",
        "        Then returns one of those children.\n",
        "        \"\"\"\n",
        "\n",
        "        assert not self.is_done, \"can't expand from terminal state\"\n",
        "\n",
        "        for action in range(n_actions):\n",
        "            self.children.add(Node(self, action))\n",
        "\n",
        "        # If you have implemented any heuristics in select_best_leaf(), they will be used here.\n",
        "        # Otherwise, this is equivalent to picking some undefined newly created child node.\n",
        "        return self.select_best_leaf()\n",
        "\n",
        "    def rollout(self, t_max=10**4):\n",
        "        \"\"\"\n",
        "        Play the game from this state to the end (done) or for t_max steps.\n",
        "\n",
        "        On each step, pick action at random (hint: env.action_space.sample()).\n",
        "\n",
        "        Compute sum of rewards from the current state until the end of the episode.\n",
        "        Note 1: use env.action_space.sample() for picking a random action.\n",
        "        Note 2: if the node is terminal (self.is_done is True), just return self.immediate_reward.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # set env into the appropriate state\n",
        "        env.load_snapshot(self.snapshot)\n",
        "        obs = self.observation\n",
        "        is_done = self.is_done\n",
        "\n",
        "        # <YOUR CODE: perform rollout and compute reward>\n",
        "        rollout_reward = 0\n",
        "        r_cnt = 0\n",
        "        while not is_done and r_cnt < t_max:\n",
        "            r_cnt += 1\n",
        "            _, r, is_done, _ = env.step(env.action_space.sample())\n",
        "            rollout_reward += r\n",
        "\n",
        "        return rollout_reward\n",
        "\n",
        "    def propagate(self, child_qvalue):\n",
        "        \"\"\"\n",
        "        Uses child Q-value (sum of rewards) to update parents recursively.\n",
        "        \"\"\"\n",
        "        # compute node Q-value\n",
        "        my_qvalue = self.immediate_reward + child_qvalue\n",
        "\n",
        "        # update qvalue_sum and times_visited\n",
        "        self.qvalue_sum += my_qvalue\n",
        "        self.times_visited += 1\n",
        "\n",
        "        # propagate upwards\n",
        "        if not self.is_root():\n",
        "            self.parent.propagate(my_qvalue)\n",
        "\n",
        "    def safe_delete(self):\n",
        "        \"\"\"safe delete to prevent memory leak in some python versions\"\"\"\n",
        "        del self.parent\n",
        "        for child in self.children:\n",
        "            child.safe_delete()\n",
        "            del child"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td591x3ucWv4"
      },
      "source": [
        "class Root(Node):\n",
        "    def __init__(self, snapshot, observation):\n",
        "        \"\"\"\n",
        "        creates special node that acts like tree root\n",
        "        :snapshot: snapshot (from env.get_snapshot) to start planning from\n",
        "        :observation: last environment observation\n",
        "        \"\"\"\n",
        "\n",
        "        self.parent = self.action = None\n",
        "        self.children = set()  # set of child nodes\n",
        "\n",
        "        # root: load snapshot and observation\n",
        "        self.snapshot = snapshot\n",
        "        self.observation = observation\n",
        "        self.immediate_reward = 0\n",
        "        self.is_done = False\n",
        "\n",
        "    @staticmethod\n",
        "    def from_node(node):\n",
        "        \"\"\"initializes node as root\"\"\"\n",
        "        root = Root(node.snapshot, node.observation)\n",
        "        # copy data\n",
        "        copied_fields = [\"qvalue_sum\", \"times_visited\", \"children\", \"is_done\"]\n",
        "        for field in copied_fields:\n",
        "            setattr(root, field, getattr(node, field))\n",
        "        return root"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GCMdt4fcWv6"
      },
      "source": [
        "## Main MCTS loop\n",
        "\n",
        "With all we implemented, MCTS boils down to a trivial piece of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVeH71_mcWv7"
      },
      "source": [
        "def plan_mcts(root, n_iters=10):\n",
        "    \"\"\"\n",
        "    builds tree with monte-carlo tree search for n_iters iterations\n",
        "    :param root: tree node to plan from\n",
        "    :param n_iters: how many select-expand-simulate-propagete loops to make\n",
        "    \"\"\"\n",
        "    for _ in range(n_iters):\n",
        "        # node = <YOUR CODE: select best leaf>\n",
        "        node = root.select_best_leaf()\n",
        "\n",
        "        if node.is_done:\n",
        "            # All rollouts from a terminal node are empty, and thus have 0 reward.\n",
        "            node.propagate(0)\n",
        "        else:\n",
        "            # Expand the best leaf. Perform a rollout from it. Propagate the results upwards.\n",
        "            # Note that here you have some leeway in choosing where to propagate from.\n",
        "            # Any reasonable choice should work.\n",
        "            \n",
        "            node_child = node.expand()\n",
        "            _reward = node_child.rollout()\n",
        "            node.propagate(_reward)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biAhJDQkcWv7"
      },
      "source": [
        "## Plan and execute\n",
        "\n",
        "Let's use our MCTS implementation to find the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHlvpB9-cWv7"
      },
      "source": [
        "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
        "root_observation = env.reset()\n",
        "root_snapshot = env.get_snapshot()\n",
        "root = Root(root_snapshot, root_observation)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cIHU9uacWv7"
      },
      "source": [
        "# plan from root:\n",
        "plan_mcts(root, n_iters=1000)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC8JRI3XcWv8"
      },
      "source": [
        "# import copy\n",
        "# saved_root = copy.deepcopy(root)\n",
        "# root = saved_root"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Dh0EGKcWv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d1be0375-4ec9-417b-83e2-85817d06fd73"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from itertools import count\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "total_reward = 0  # sum of rewards\n",
        "test_env = loads(root_snapshot)  # env used to show progress\n",
        "\n",
        "for i in count():\n",
        "\n",
        "    # get best child\n",
        "    # best_child = <YOUR CODE: select child with the highest mean reward>\n",
        "    children = list(root.children)\n",
        "    best_child = children[np.argmax([child.get_qvalue_estimate() for child in children])]\n",
        "\n",
        "    # take action\n",
        "    s, r, done, _ = test_env.step(best_child.action)\n",
        "\n",
        "    # show image\n",
        "    clear_output(True)\n",
        "    plt.title(\"step %i\" % i)\n",
        "    plt.imshow(test_env.render('rgb_array'))\n",
        "    plt.show()\n",
        "\n",
        "    total_reward += r\n",
        "    if done:\n",
        "        print(\"Finished with reward = \", total_reward)\n",
        "        break\n",
        "\n",
        "    # discard unrealized part of the tree [because not every child matters :(]\n",
        "    for child in root.children:\n",
        "        if child != best_child:\n",
        "            child.safe_delete()\n",
        "\n",
        "    # declare best child a new root\n",
        "    root = Root.from_node(best_child)\n",
        "\n",
        "    # assert not root.is_leaf(), \\\n",
        "    #     \"We ran out of tree! Need more planning! Try growing the tree right inside the loop.\"\n",
        "\n",
        "    # You may want to run more planning here\n",
        "    # <YOUR CODE>\n",
        "    if root.is_leaf:\n",
        "        plan_mcts(root)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVtElEQVR4nO3df7DddX3n8eeLEBLwBxG5xpgEQiGti04b3Svi6uxSHFpk7aI71sJapC6ddEec6qzjFtyu4qx0dVmldeqyxoEF1x+IvyDDUBWRmeqqwEUhJmBKkLAkJiT8Bq0pCe/943yDx3BDzv2Vm8+9z8fMmfP9fj6f7znvjx5efPmc7/eeVBWSpHYcNN0FSJLGxuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG41bwkFyT57BS+/ruSjCTZkeTyUfr/NMmGJE8k+XqSl/T1LUhyRZJt3eOCqapTs4fBLe3bz4APA5ft2ZHkJOCvgNOBI4B7gC/0DbkYOAxYBpwAnJXkHVNbrmY6g1vNSPIXSTYneTzJ+iSvT3Iq8H7gj7oz3tu7sYcnuTTJlu6YDyeZ0/X9SZL/m+Rvkzya5CdJXr+3962qr1bV1cCDo3S/EfhSVa2rqn8C/ivwL5Mc2/X/AfDfq+oXVbURuBT495P1v4lmp4OnuwBpEEl+C3gX8Kqq+lmSZcCcqro7yV8Bx1XVH/cdcjmwDTgOeA5wLXAf8Kmu/9XAl4EjgX8LfDXJMVX10HjKG2X75cDde+l/+TjeQ3qaZ9xqxS5gHnB8krlVtbGq7h5tYJKFwGnAe6rq51W1jd6SxRl9w7YBf11VT1bVF4H1wL8eR11fB96a5LeTHAp8ACh6yyO7+89L8rwkx9E72z5s9JeSBmNwqwlVtQF4D3ABsC3Jlf1fAu7haGAusCXJI0keoXem/aK+MZvr1//C2r3A3l7v2er6FvBB4CvAxu7xOLCpG/LnwD8CdwHX0Fv/3rTn60hjYXCrGVX1+ap6Hb1gLuCju7v2GHofsAM4sqoWdI/nV9XL+sYsTtK/hHEUvS8hx1PXJ6tqeVUtpBfgBwNru76HquptVfXi7v0PAm4ez/tIuxncakKS30pycpJ5wC/pncU+1XXfDyxLchBAVW0Bvgl8LMnzkxyU5Ngk/6rvJV8E/HmSuUn+EPhnwHV7ee+Dk8wH5gBzksxPcnDXNz/Jy9NzFLAK+JuqerjrPzbJC5PMSfIGYCW9K1SkcTO41Yp5wEeAB4Ct9IL3/K7vS93zg0l+2G2/HTgEuAN4mN4XkYv6Xu8mYHn3ehcCb6mq0a4aAfhLev+iOA/44277L7u++cDngSfonUl/H/gvfcf+c+DH9JZP/hvwtqpaN4Z5S88Qf0hBs02SPwH+tFt2kZrjGbckNWbKgjvJqd1NEhuSnDdV7yNJs82ULJV0d6j9A3AKvUufbgHOrKo7Jv3NJGmWmaoz7hOADVX10+424Cvp/S0HSdIETdUt74vpXUu72yZ6txiP6sgjj6xly5ZNUSmS1J6NGzfywAMPZLS+aftbJUlW0rumlaOOOoqRkZHpKkWSDjjDw8N77ZuqpZLNwNK+/SVd29OqalVVDVfV8NDQ0BSVIUkzz1QF9y3A8iTHJDmE3h/3WT1F7yVJs8qULJVU1c4k7wK+Qe824cu8W0ySJseUrXFX1XXs5W8/SJLGzzsnJakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMZM6Dcnk2wEHgd2ATurajjJEcAXgWXARuCtVfXwxMqUJO02GWfcv1tVK6pquNs/D7ihqpYDN3T7kqRJMhVLJacDV3TbVwBvmoL3kKRZa6LBXcA3k9yaZGXXtrCqtnTbW4GFox2YZGWSkSQj27dvn2AZkjR7TGiNG3hdVW1O8iLg+iQ/6e+sqkpSox1YVauAVQDDw8OjjpEkPdOEzriranP3vA34GnACcH+SRQDd87aJFilJ+pVxB3eS5yR53u5t4PeAtcBq4Oxu2NnANRMtUpL0KxNZKlkIfC3J7tf5fFV9PcktwFVJzgHuBd468TIlSbuNO7ir6qfA74zS/iDw+okUJUnaO++clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUmH0Gd5LLkmxLsrav7Ygk1ye5q3t+QdeeJJ9IsiHJmiSvnMriJWk2GuSM+3Lg1D3azgNuqKrlwA3dPsAbgOXdYyVwyeSUKUnabZ/BXVV/Dzy0R/PpwBXd9hXAm/raP1M9PwAWJFk0WcVKksa/xr2wqrZ021uBhd32YuC+vnGburZnSLIyyUiSke3bt4+zDEmafSb85WRVFVDjOG5VVQ1X1fDQ0NBEy5CkWWO8wX3/7iWQ7nlb174ZWNo3bknXJkmaJOMN7tXA2d322cA1fe1v764uORF4tG9JRZI0CQ7e14AkXwBOAo5Msgn4IPAR4Kok5wD3Am/thl8HnAZsAH4BvGMKapakWW2fwV1VZ+6l6/WjjC3g3IkWJUnaO++clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUmH0Gd5LLkmxLsrav7YIkm5Pc1j1O6+s7P8mGJOuT/P5UFS5Js9UgZ9yXA6eO0n5xVa3oHtcBJDkeOAN4WXfM/0wyZ7KKlSQNENxV9ffAQwO+3unAlVW1o6ruofdr7ydMoD5J0h4mssb9riRruqWUF3Rti4H7+sZs6tqeIcnKJCNJRrZv3z6BMiRpdhlvcF8CHAusALYAHxvrC1TVqqoarqrhoaGhcZYhSbPPuIK7qu6vql1V9RTwaX61HLIZWNo3dEnXJkmaJOMK7iSL+nbfDOy+4mQ1cEaSeUmOAZYDN0+sRElSv4P3NSDJF4CTgCOTbAI+CJyUZAVQwEbgzwCqal2Sq4A7gJ3AuVW1a2pKl6TZaZ/BXVVnjtJ86bOMvxC4cCJFSZL2zjsnJakxBrckNcbglqTGGNyS1BiDW5Ias8+rSqQD3VO7dvLzbT+FKg4bWsacufOmuyRpShncatLPt93Dz0ZWA1BP7eLxLXdBFce/5QMcesRLprk6aWoZ3GrSk//4OI9tumOP1kxLLdL+5hq3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMbsM7iTLE1yY5I7kqxL8u6u/Ygk1ye5q3t+QdeeJJ9IsiHJmiSvnOpJSNJsMsgZ907gvVV1PHAicG6S44HzgBuqajlwQ7cP8AZ6v+6+HFgJXDLpVUvSLLbP4K6qLVX1w277ceBOYDFwOnBFN+wK4E3d9unAZ6rnB8CCJIsmvXJJmqXGtMadZBnwCuAmYGFVbem6tgILu+3FwH19h23q2vZ8rZVJRpKMbN++fYxlS9LsNXBwJ3ku8BXgPVX1WH9fVRVQY3njqlpVVcNVNTw0NDSWQyVpVhsouJPMpRfan6uqr3bN9+9eAumet3Xtm4GlfYcv6dokSZNgkKtKAlwK3FlVH+/rWg2c3W2fDVzT1/727uqSE4FH+5ZUJEkTNMgv4LwWOAv4cZLburb3Ax8BrkpyDnAv8Nau7zrgNGAD8AvgHZNasSTNcvsM7qr6Lnv/TajXjzK+gHMnWJckaS+8c1KSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMG+bHgpUluTHJHknVJ3t21X5Bkc5Lbusdpfcecn2RDkvVJfn8qJyBJs80gPxa8E3hvVf0wyfOAW5Nc3/VdXFX/o39wkuOBM4CXAS8BvpXkN6tq12QWLkmz1T7PuKtqS1X9sNt+HLgTWPwsh5wOXFlVO6rqHnq/9n7CZBQrSRrjGneSZcArgJu6pnclWZPksiQv6NoWA/f1HbaJZw96SdIYDBzcSZ4LfAV4T1U9BlwCHAusALYAHxvLGydZmWQkycj27dvHcqjEnLnzyJy5e7QWO3c8MS31SPvTQMGdZC690P5cVX0VoKrur6pdVfUU8Gl+tRyyGVjad/iSru3XVNWqqhququGhoaGJzEGz0HMX/SbPGTr6Ge33r7l+lNHSzDLIVSUBLgXurKqP97Uv6hv2ZmBtt70aOCPJvCTHAMuBmyevZAl6H8s8s6Nqv9ci7W+DXFXyWuAs4MdJbuva3g+cmWQFUMBG4M8AqmpdkquAO+hdkXKuV5RI0uTZZ3BX1XcZ9dSG657lmAuBCydQlyRpL7xzUpIaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTGD/FlXab+56KKL+N73vjfQ2LNevYCjjzjk19pGRkZ436fePNDxp5xyCu985zvHXKM03QxuHVBuueUWrr766oHGnnbsH/CSBUfzVM0B4KDsZOvWe7n66m8MdLy/vKRWGdxq1i92PY/vP/hGfr7rcACef/BD7Hjq09NclTT1XONWs9Y+9i94bOcL2VVz2VVzefjJF7Htl0dNd1nSlDO41aydTx3Cr/84U9jyy2OmqxxpvzG41azD5jxO7ydPdyuWPeeO6SpH2m8G+ZX3+UluTnJ7knVJPtS1H5PkpiQbknwxySFd+7xuf0PXv2xqp6DZ6mWHf58Xz7+HQ/MQDz54L08+OsKTP98w3WVJU26QLyd3ACdX1RNJ5gLfTfJ3wH8ELq6qK5P8L+Ac4JLu+eGqOi7JGcBHgT+aovo1i135rZs58vB17PinXVx/60/ZuWsXv34GLs1Mg/zKewFPdLtzu0cBJwP/rmu/AriAXnCf3m0DfBn42yTpXmdUW7du5aKLLhpH+Zpp1q9fP/DY76z5fxN6r9tvv93PnQ5YW7du3WvfQJcDJpkD3AocB3wSuBt4pKp2dkM2AYu77cXAfQBVtTPJo8ALgQf2eM2VwEqAxYsXc9ZZZw04Hc1k3/nOd1izZs1+ea/ly5f7udMB67Of/exe+wYK7qraBaxIsgD4GvDSiRZVVauAVQDDw8P14he/eKIvqRlg/vz5++29DjvsMPzc6UA1d+7cvfaN6aqSqnoEuBF4DbAgye7gXwJs7rY3A0sBuv7DgQfHVrIkaW8GuapkqDvTJsmhwCnAnfQC/C3dsLOBa7rt1d0+Xf+3n219W5I0NoMslSwCrujWuQ8Crqqqa5PcAVyZ5MPAj4BLu/GXAv8nyQbgIeCMKahbkmatQa4qWQO8YpT2nwInjNL+S+APJ6U6SdIzeOekJDXGvw6oA8qrXvUqnnzyyf3yXitWrNgv7yNNNoNbB5T3ve99012CdMBzqUSSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGDPJjwfOT3Jzk9iTrknyoa788yT1JbuseK7r2JPlEkg1J1iR55VRPQpJmk0F+SGEHcHJVPZFkLvDdJH/X9b2vqr68x/g3AMu7x6uBS7pnSdIk2OcZd/U80e3O7R71LIecDnymO+4HwIIkiyZeqiQJBlzjTjInyW3ANuD6qrqp67qwWw65OMm8rm0xcF/f4Zu6tj1fc2WSkSQj27dvn8AUJGl2GSi4q2pXVa0AlgAnJHk5cD7wUuBVwBHAX4zljatqVVUNV9Xw0NDQGMuWpNlrTFeVVNUjwI3AqVW1pVsO2QH8b+CEbthmYGnfYUu6NknSJBjkqpKhJAu67UOBU4Cf7F63ThLgTcDa7pDVwNu7q0tOBB6tqi1TUr0kzUKDXFWyCLgiyRx6QX9VVV2b5NtJhoAAtwH/oRt/HXAasAH4BfCOyS9bkmavfQZ3Va0BXjFK+8l7GV/AuRMvTZI0Gu+clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWpMej/KPs1FJI8D66e7jilyJPDAdBcxBWbqvGDmzs15teXoqhoarePg/V3JXqyvquHpLmIqJBmZiXObqfOCmTs35zVzuFQiSY0xuCWpMQdKcK+a7gKm0Eyd20ydF8zcuTmvGeKA+HJSkjS4A+WMW5I0IINbkhoz7cGd5NQk65NsSHLedNczVkkuS7Itydq+tiOSXJ/kru75BV17knyim+uaJK+cvsqfXZKlSW5MckeSdUne3bU3Pbck85PcnOT2bl4f6tqPSXJTV/8XkxzStc/r9jd0/cums/59STInyY+SXNvtz5R5bUzy4yS3JRnp2pr+LE7EtAZ3kjnAJ4E3AMcDZyY5fjprGofLgVP3aDsPuKGqlgM3dPvQm+fy7rESuGQ/1TgeO4H3VtXxwInAud3/N63PbQdwclX9DrACODXJicBHgYur6jjgYeCcbvw5wMNd+8XduAPZu4E7+/ZnyrwAfreqVvRds936Z3H8qmraHsBrgG/07Z8PnD+dNY1zHsuAtX3764FF3fYiejcYAXwKOHO0cQf6A7gGOGUmzQ04DPgh8Gp6d94d3LU//bkEvgG8pts+uBuX6a59L/NZQi/ATgauBTIT5tXVuBE4co+2GfNZHOtjupdKFgP39e1v6tpat7CqtnTbW4GF3XaT8+3+M/oVwE3MgLl1ywm3AduA64G7gUeqamc3pL/2p+fV9T8KvHD/Vjywvwb+E/BUt/9CZsa8AAr4ZpJbk6zs2pr/LI7XgXLL+4xVVZWk2WsukzwX+Arwnqp6LMnTfa3Orap2ASuSLAC+Brx0mkuasCRvBLZV1a1JTprueqbA66pqc5IXAdcn+Ul/Z6ufxfGa7jPuzcDSvv0lXVvr7k+yCKB73ta1NzXfJHPphfbnquqrXfOMmBtAVT0C3EhvCWFBkt0nMv21Pz2vrv9w4MH9XOogXgv8myQbgSvpLZf8De3PC4Cq2tw9b6P3L9sTmEGfxbGa7uC+BVjeffN9CHAGsHqaa5oMq4Gzu+2z6a0P725/e/et94nAo33/qXdASe/U+lLgzqr6eF9X03NLMtSdaZPkUHrr9nfSC/C3dMP2nNfu+b4F+HZ1C6cHkqo6v6qWVNUyev8cfbuq3kbj8wJI8pwkz9u9DfwesJbGP4sTMt2L7MBpwD/QW2f8z9Ndzzjq/wKwBXiS3lraOfTWCm8A7gK+BRzRjQ29q2juBn4MDE93/c8yr9fRW1dcA9zWPU5rfW7AbwM/6ua1FvhA1/4bwM3ABuBLwLyufX63v6Hr/43pnsMAczwJuHamzKubw+3dY93unGj9sziRh7e8S1JjpnupRJI0Rga3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5Jasz/BwvMVkhjthAAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished with reward =  200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fypqn8scWv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8e06fc-c21a-4a84-8906-f9ceb142304f"
      },
      "source": [
        "from submit import submit_mcts\n",
        "\n",
        "submit_mcts(total_reward, 'your.email@example.com', 'YourAssignmentToken')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaUP3vFMcWv8"
      },
      "source": [
        "## Bonus assignments (10+pts each)\n",
        "\n",
        "There's a few things you might want to try if you want to dig deeper:\n",
        "\n",
        "### Node selection and expansion\n",
        "\n",
        "\"Analyze this\" assignment\n",
        "\n",
        "UCB-1 is a weak bound as it relies on a very general bounds (Hoeffding Inequality, to be exact). \n",
        "* Try playing with the exploration parameter $C_p$. The theoretically optimal $C_p$ you can get from a max reward of the environment (max reward for CartPole is 200).\n",
        "* Use using a different exploration strategy (bayesian UCB, for example)\n",
        "* Expand not all but several random actions per `expand` call. See __the notes below__ for details.\n",
        "\n",
        "The goal is to find out what gives the optimal performance for `CartPole-v0` for different time budgets (i.e. different n_iter in plan_mcts.)\n",
        "\n",
        "Evaluate your results on `Acrobot-v1` - do the results change and if so, how can you explain it?\n",
        "\n",
        "\n",
        "### Atari-RAM\n",
        "\n",
        "\"Build this\" assignment\n",
        "\n",
        "Apply MCTS to play Atari games. In particular, let's start with ```gym.make(\"MsPacman-ramDeterministic-v0\")```.\n",
        "\n",
        "This requires two things:\n",
        "* Slightly modify WithSnapshots wrapper to work with atari.\n",
        "\n",
        " * Atari has a special interface for snapshots:\n",
        "   ```   \n",
        "   snapshot = self.env.ale.cloneState()\n",
        "   ...\n",
        "   self.env.ale.restoreState(snapshot)\n",
        "   ```\n",
        " * Try it on the env above to make sure it does what you told it to.\n",
        " \n",
        "* Run MCTS on the game above. \n",
        " * Start with small tree size to speed-up computations\n",
        " * You will probably want to rollout for 10-100 steps (t_max) for starters\n",
        " * Consider using discounted rewards (see __notes at the end__)\n",
        " * Try a better rollout policy\n",
        " \n",
        " \n",
        "### Integrate learning into planning\n",
        "\n",
        "Planning on each iteration is a costly thing to do. You can speed things up drastically if you train a classifier to predict which action will turn out to be best according to MCTS.\n",
        "\n",
        "To do so, just record which action did the MCTS agent take on each step and fit something to [state, mcts_optimal_action]\n",
        "* You can also use optimal actions from discarded states to get more (dirty) samples. Just don't forget to fine-tune without them.\n",
        "* It's also worth a try to use P(best_action|state) from your model to select best nodes in addition to UCB\n",
        "* If your model is lightweight enough, try using it as a rollout policy.\n",
        "\n",
        "While CartPole is glorious enough, try expanding this to ```gym.make(\"MsPacmanDeterministic-v0\")```\n",
        "* See previous section on how to wrap atari\n",
        "\n",
        "* Also consider what [AlphaGo Zero](https://deepmind.com/blog/alphago-zero-learning-scratch/) did in this area.\n",
        "\n",
        "### Integrate planning into learning \n",
        "_(this will likely take long time, better consider this as side project when all other deadlines are met)_\n",
        "\n",
        "Incorporate planning into the agent architecture. The goal is to implement [Value Iteration Networks](https://arxiv.org/abs/1602.02867).\n",
        "\n",
        "Remember [week5 assignment](https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_a3c.ipynb)? You will need to switch it into a maze-like game, like MsPacman, and implement a special layer that performs value iteration-like update to a recurrent memory. This can be implemented the same way you did in the POMDP assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6znrmZpcWv9"
      },
      "source": [
        "## Notes\n",
        "\n",
        "\n",
        "#### Assumptions\n",
        "\n",
        "The full list of assumptions is:\n",
        "\n",
        "* __Finite number of actions__: we enumerate all actions in `expand`.\n",
        "* __Episodic (finite) MDP__: while technically it works for infinite MDPs, we perform a rollout for $10^4$ steps. If you are knowingly infinite, please adjust `t_max` to something more reasonable.\n",
        "* __Deterministic MDP__: `Node` represents the single outcome of taking `self.action` in `self.parent`, and does not support the situation where taking an action in a state may lead to different rewards and next states.\n",
        "* __No discounted rewards__: we assume $\\gamma=1$. If that isn't the case, you only need to change two lines in `rollout()` and use `my_qvalue = self.immediate_reward + gamma * child_qvalue` for `propagate()`.\n",
        "* __pickleable env__: won't work if e.g. your env is connected to a web-browser surfing the internet. For custom envs, you may need to modify get_snapshot/load_snapshot from `WithSnapshots`.\n",
        "\n",
        "#### On `get_best_leaf` and `expand` functions\n",
        "\n",
        "This MCTS implementation only selects leaf nodes for expansion.\n",
        "This doesn't break things down because `expand` adds all possible actions. Hence, all non-leaf nodes are by design fully expanded and shouldn't be selected.\n",
        "\n",
        "If you want to only add a few random action on each expand, you will also have to modify `get_best_leaf` to consider returning non-leafs.\n",
        "\n",
        "#### Rollout policy\n",
        "\n",
        "We use a simple uniform policy for rollouts. This introduces a negative bias to good situations that can be messed up completely with random bad action. As a simple example, if you tend to rollout with uniform policy, you better don't use sharp knives and walk near cliffs.\n",
        "\n",
        "You can improve that by integrating a reinforcement _learning_ algorithm with a computationally light agent. You can even train this agent on optimal policy found by the tree search.\n",
        "\n",
        "#### Contributions\n",
        "* Reusing some code from 5vision [solution for deephack.RL](https://github.com/5vision/uct_atari), code by Mikhail Pavlov\n",
        "* Using some code from [this gist](https://gist.github.com/blole/dfebbec182e6b72ec16b66cc7e331110)\n",
        "\n",
        "#### References\n",
        "* <a id=\"1\">[1]</a> _Coulom R. (2007) Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In: van den Herik H.J., Ciancarini P., Donkers H.H.L.M.. (eds) Computers and Games. CG 2006. Lecture Notes in Computer Science, vol 4630. Springer, Berlin, Heidelberg_\n",
        "\n",
        "* <a id=\"2\">[2]</a> _Kocsis L., Szepesvári C. (2006) Bandit Based Monte-Carlo Planning. In: Fürnkranz J., Scheffer T., Spiliopoulou M. (eds) Machine Learning: ECML 2006. ECML 2006. Lecture Notes in Computer Science, vol 4212. Springer, Berlin, Heidelberg_\n",
        "\n",
        "* <a id=\"3\">[3]</a> _Kocsis, Levente, Csaba Szepesvári, and Jan Willemson. \"Improved monte-carlo search.\" Univ. Tartu, Estonia, Tech. Rep 1 (2006)._\n",
        "\n",
        "* <a id=\"4\">[4]</a> _C. B. Browne et al., \"A Survey of Monte Carlo Tree Search Methods,\" in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012, doi: 10.1109/TCIAIG.2012.2186810._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdFV9sTKcWv9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}